Two types of data types : System defined datatypes (Primitives) & User-defined datatypes.
Primitive types include int, float, char, double etc. the number of bytes allocated for datatype varies depending on the compiler.
In cpp, user defined datatypes are achieved through structs / classes.
with data available in memory, data structure is a way of organizing the data in memory efficiently so that it becomes 
easy to manipulate them to achieve desired objectives. Data structures include linked lists,stack, arrays,files, trees, graphs.

Linear data structure : Elements are accessed in sequential order but it is not compulsory to store them sequentially . e.g linked list, array, stack, queue
Non-linear data structure : Elements are accessed/stored in non-linear fashion. Examples : Trees , graphs

Abstract data types : Combining the user defined data structures with their operations so that the data can be manipulated
through that adt's methods. For example , stack data structure involves operations of pushing, popping etc.

Algorithm : step-by-step unambiguous instructions to solve a given problem. main criteria for judging an algorithm:
correctness : does the algorithm reach to the correct solution in finite number of steps.
efficiency : the resources (memory and time) taken to reach the correct solution. 
Algorithm analysis helps rate algorithms based on efficiency so that a decision can be made as to which algorithm should be used.

Running time analysis : how processing time increases with increase in size of the inputs - could be size of array, number
of elements in matrix, number of bits in bitstream , vertices and edges in graphs/trees etc

Comparison of algorithms :
How to measure the efficienty of algorithm :
a. Execution time : not the ideal measure as it is dependent on compiler, platform, HW etc
b. Number of statements executed : not an ideal measure as it depends on language used and sometimes an algorithm with more statements could be more efficient.

To have a metric independent of machine, language etc, we could derive a function f(n) which lets us know how the running time
scales with the size of inputs.

Rate of growth : the scale at which the running time increases with increase in input size.
Commonly used rate of growth:
2^(2^n) , n!, 4^n, 2^n, n^2, nlog(n), log(n!),n,2^(log(n)), log^2(n), sqrt(log(n)), log(log(n)), 1.

Case analysis:
Worst case : Inputs for which the algorithm takes a long time to complete and is slowest to execute.
Best case : Inputs for which the algorithm takes very little time to complete and is fastest to execute.
Average case : Run different kinds of inputs and check the distribution of run times over all these inputs. Take an average of runtimes for these samples.this gives 
a prediction of how the algorithm might run on new inputs.

Best case runtime <= Average case runtime <= Worst case runtime.

Asymptotic notation : 
For having expressions for best case, average case, worst case, upper and lower bounds need to be identified.
It is represented in the form of function f(n) of inputs.

Big O notation:
This notation gives tight upper bound of the given function. f(n) = O(g(n)). At larger values of n, upper bound of f(n) is g(n).

O-notation defined as O(g(n)) = {f(n): there exists positive constants c and n0 such that 0<= f(n) <= cg(n) for n > n0}.
Objective is to give smallest rate of growth which is greatert than the algorithm's worst case growing rate. Below n0, the rate of growth could be greater than g(n).

Examples :
1. Find upper bound for 3n+8
Soln : 3n+8 <= 4n for n >=8, hence c = 4, n0 = 8 , cf(n) = 4n.
2. Find upper bound for n^2+1,
Soln : n^2 + 1 <= 2n^2 , for n>=1, c = 2, n0 = 1.

3. Find upper bound for f(n) = n^4 + 100n^2 + 50
Soln : f(n) <= 2n^4 for n >=11, c = 2, n0 = 11
4. Find upper bound for f(n) = 2n^3 – 2n^2
Soln : f(n) <= 2n^3, for n >= 1, c = 2, n0=1  O(n^3)

5. Find upper bound for f(n) = n
soln : n >= n, for n >= 1, c = 1, n0 = 1, O(n) complexity

6. Find upper bound for f(n) = 410
410 >= 410 for any n, O(1) complexity.

There is no unique set of values for n0 and c in proving the asymptotic bounds. for example,
100n + 5= O(n)
100n + 5 <= 100n + n  = 101n for n>=5 => c = 101, n0=5.
at the same time, 100n + 5 <= 100n + 5n, for all n0>1 =>c-105, n0=1

Omega Q notation:
This gives the tighter lower bound of the algorithm f(n) = omega(g(n)) => meaning f(n) can never be better than g(n) in terms of complexity.

Example : 
1.100n^2 + 10n + 50 >= 100n^2 => omega(n^2).

Theta notation:
The notation decides whether the upper and lower bounds are the same.if they are of same complexity, it means
the average time complexity is of the same order. if the big O and omega notations are not the same, the average notation 
Theta needs to computed for different cases and averaged.

theta(g(n)) = {f(n) , there exists positive constants c1, c2 and n0 such that 0<=c1g(n) <= f(n) <= c2g(n) for all n >= n0} 
 
 Example:
 1.Find theta bound for f(n) = n^2/2 - n/2.
Soln. n^2/5 <= n^2/2 - n/2 <= n^2/2 , for n>=2, c1 = 1/5, c2 = 1/2, n0=2

2. Prove n ≠ Θ(n^2)
let us assume c1n^2 <= n <= c2n^2 for n>=n0
divide by n throughout, assuming n > 0
c1 * n <= 1 <= c2 * n, 
n <= 1/c1 , for all n which does not satisfy this condition, the statement n = theta(n^2) does not hold true





