Two types of data types : System defined datatypes (Primitives) & User-defined datatypes.
Primitive types include int, float, char, double etc. the number of bytes allocated for datatype varies depending on the compiler.
In cpp, user defined datatypes are achieved through structs / classes.
with data available in memory, data structure is a way of organizing the data in memory efficiently so that it becomes 
easy to manipulate them to achieve desired objectives. Data structures include linked lists,stack, arrays,files, trees, graphs.

Linear data structure : Elements are accessed in sequential order but it is not compulsory to store them sequentially . e.g linked list, array, stack, queue
Non-linear data structure : Elements are accessed/stored in non-linear fashion. Examples : Trees , graphs

Abstract data types : Combining the user defined data structures with their operations so that the data can be manipulated
through that adt's methods. For example , stack data structure involves operations of pushing, popping etc.

Algorithm : step-by-step unambiguous instructions to solve a given problem. main criteria for judging an algorithm:
correctness : does the algorithm reach to the correct solution in finite number of steps.
efficiency : the resources (memory and time) taken to reach the correct solution. 
Algorithm analysis helps rate algorithms based on efficiency so that a decision can be made as to which algorithm should be used.

Running time analysis : how processing time increases with increase in size of the inputs - could be size of array, number
of elements in matrix, number of bits in bitstream , vertices and edges in graphs/trees etc

Comparison of algorithms :
How to measure the efficienty of algorithm :
a. Execution time : not the ideal measure as it is dependent on compiler, platform, HW etc
b. Number of statements executed : not an ideal measure as it depends on language used and sometimes an algorithm with more statements could be more efficient.

To have a metric independent of machine, language etc, we could derive a function f(n) which lets us know how the running time
scales with the size of inputs.

Rate of growth : the scale at which the running time increases with increase in input size.
Commonly used rate of growth:
2^(2^n) , n!, 4^n, 2^n, n^2, nlog(n), log(n!),n,2^(log(n)), log^2(n), sqrt(log(n)), log(log(n)), 1.

Case analysis:
Worst case : Inputs for which the algorithm takes a long time to complete and is slowest to execute.
Best case : Inputs for which the algorithm takes very little time to complete and is fastest to execute.
Average case : Run different kinds of inputs and check the distribution of run times over all these inputs. Take an average of runtimes for these samples.this gives 
a prediction of how the algorithm might run on new inputs.

Best case runtime <= Average case runtime <= Worst case runtime.

Asymptotic notation : 
For having expressions for best case, average case, worst case, upper and lower bounds need to be identified.
It is represented in the form of function f(n) of inputs.

Big O notation:
This notation gives tight upper bound of the given function. f(n) = O(g(n)). At larger values of n, upper bound of f(n) is g(n).

O-notation defined as O(g(n)) = {f(n): there exists positive constants c and n0 such that 0<= f(n) <= cg(n) for n > n0}.
Objective is to give smallest rate of growth which is greatert than the algorithm's worst case growing rate. Below n0, the rate of growth could be greater than g(n).

Examples :
1. Find upper bound for 3n+8
Soln : 3n+8 <= 4n for n >=8, hence c = 4, n0 = 8 , cf(n) = 4n.
2. Find upper bound for n^2+1,
Soln : n^2 + 1 <= 2n^2 , for n>=1, c = 2, n0 = 1.

3. Find upper bound for f(n) = n^4 + 100n^2 + 50
Soln : f(n) <= 2n^4 for n >=11, c = 2, n0 = 11
4. Find upper bound for f(n) = 2n^3 – 2n^2
Soln : f(n) <= 2n^3, for n >= 1, c = 2, n0=1  O(n^3)

5. Find upper bound for f(n) = n
soln : n >= n, for n >= 1, c = 1, n0 = 1, O(n) complexity

6. Find upper bound for f(n) = 410
410 >= 410 for any n, O(1) complexity.

There is no unique set of values for n0 and c in proving the asymptotic bounds. for example,
100n + 5= O(n)
100n + 5 <= 100n + n  = 101n for n>=5 => c = 101, n0=5.
at the same time, 100n + 5 <= 100n + 5n, for all n0>1 =>c-105, n0=1

Omega Q notation:
This gives the tighter lower bound of the algorithm f(n) = omega(g(n)) => meaning f(n) can never be better than g(n) in terms of complexity.

Example : 
1.100n^2 + 10n + 50 >= 100n^2 => omega(n^2).

Theta notation:
The notation decides whether the upper and lower bounds are the same.if they are of same complexity, it means
the average time complexity is of the same order. if the big O and omega notations are not the same, the average notation 
Theta needs to computed for different cases and averaged.

theta(g(n)) = {f(n) , there exists positive constants c1, c2 and n0 such that 0<=c1g(n) <= f(n) <= c2g(n) for all n >= n0} 
 
 Example:
 1.Find theta bound for f(n) = n^2/2 - n/2.
Soln. n^2/5 <= n^2/2 - n/2 <= n^2/2 , for n>=2, c1 = 1/5, c2 = 1/2, n0=2

2. Prove n ≠ Θ(n^2)
let us assume c1n^2 <= n <= c2n^2 for n>=n0
divide by n throughout, assuming n > 0
c1 * n <= 1 <= c2 * n, 
n <= 1/c1 , for all n which does not satisfy this condition, the statement n = theta(n^2) does not hold true.

Usually for analysis, we care about the upper bound because we usually analyze the worst running time of the algorithm
and lower bound for an algorithm is of little practical importance. In case when worst and best cases are of the same complexity,
we use average running time.

The upper bound or lower bound acts as an asymptote, a mathematical curve to which the worst case or best case runtime 
is approximated to at higher values of n[the size of input].Hence, the name asymptotic analysis.

Guidelines of asymptotic analysis:

Loops : Running time of for loop is at most the running time of statements multiplied by the number of iterations.
for (i=1;i<=n;i++)
{
    m = m+2; // constant time c
}
So, runtime =  c * n => O(n)

Nested for loops:
Analyze from the inside out. Total running time is the product of the
sizes of all the loops.
for (i=1;i<=n;i++)
{
    // inner loop executes n times.
    for (j=1;j<=n;j++)
    {
        k=k+1; // constant time
    }
}

Total time = c * n * n = c * n^2 = O(n^2)

Consecutive statements : Add the time complexities of each of the individual statements.

x = x + 1; // constant time
for (i=1;i<=n;i++)
{
    m = m+2; //constant time
}
for (i=1;i<=n;i++)
{
    for (j=1;j<=n;j++)
    {
        n = n+1; // constant time 
    }
}

time = c0 + c1 * n + c2 * n^2 = O(n^2)

If-then else statements : Worst case running part : the test case + the bigger part of if or else.

if (length() == 0)
{
    return false; // then part : constant
}

else{ // (constant + constant) * n
    for(int n=0; n<length();n++)
    {  // constant + constant
       if (!list[n].equals(otherList.list[n]))
           return false;
    }
}
Worst case runtime : c0[test] + (c2+c3) * n [inside else] = O(n)

Logarithmic complexity:
an algorithm is O(n) if it takes a constant time to cut the problem size by fraction (usually by 1/2) .Consider the program:
for(i=1;i<=n;)
{
   i = 2*i;
}
the value of i keeps increasing by multiples of 2 such as 1,2,4,8... Let us say at k th step, 2^k = n, at k+1, 
the program comes out of the loop. taking logarithm on both sides,
2^k = n
log_2 (2^k) = log_2 (n)
k = log_2 (n)
Time = O(log_2(n))

The same analysis holds for cases where i = i / 2, where i decrements by a fraction and it constant time to do so every iteration.









